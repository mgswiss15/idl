{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning - Week 1 Exercises SOLUTIONS\n",
    "## Neural Network Basics with PyTorch\n",
    "\n",
    "This notebook contains complete solutions to all Week 1 exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Basics\n",
    "\n",
    "### Exercise 1.1: Creating and Manipulating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Create a tensor from a list\n",
    "tensor_a = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "print(f\"Shape: {tensor_a.shape}\")\n",
    "print(f\"Data type: {tensor_a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Create a 3x4 tensor with random values from standard normal distribution\n",
    "tensor_b = torch.randn(3, 4)\n",
    "\n",
    "print(f\"Tensor shape: {tensor_b.shape}\")\n",
    "print(tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Create tensors of ones and zeros\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "\n",
    "print(\"Ones tensor:\")\n",
    "print(ones_tensor)\n",
    "print(\"\\nZeros tensor:\")\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Reshape tensors\n",
    "reshaped_1 = tensor_b.reshape(2, 6)\n",
    "reshaped_2 = tensor_b.reshape(12, 1)\n",
    "# Alternative: tensor_b.view(2, 6) or tensor_b.view(12, 1)\n",
    "\n",
    "print(f\"Original shape: {tensor_b.shape}\")\n",
    "print(f\"Reshaped to (2, 6): {reshaped_1.shape}\")\n",
    "print(f\"Reshaped to (12, 1): {reshaped_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Dot product\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "w = torch.tensor([0.5, -0.3, 0.8])\n",
    "\n",
    "dot_product = torch.dot(x, w)\n",
    "\n",
    "print(f\"Dot product: {dot_product}\")\n",
    "print(f\"Manual verification: 1*0.5 + 2*(-0.3) + 3*0.8 = {1*0.5 + 2*(-0.3) + 3*0.8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Matrix-vector multiplication\n",
    "W = torch.randn(3, 2)\n",
    "x = torch.randn(2)\n",
    "\n",
    "result = torch.matmul(W, x)\n",
    "# Alternative: result = W @ x\n",
    "\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Broadcasting\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(2, 3)\n",
    "C = torch.randn(3)\n",
    "\n",
    "sum_AB = A + B  # Element-wise addition\n",
    "sum_AC = A + C  # Broadcasting: C is broadcast to (2, 3)\n",
    "\n",
    "print(f\"A + B shape: {sum_AB.shape}\")\n",
    "print(f\"A + C shape: {sum_AC.shape}\")\n",
    "print(\"\\nBroadcasting example:\")\n",
    "print(f\"A shape: {A.shape}, C shape: {C.shape}\")\n",
    "print(\"C is broadcast to match A's shape!\")\n",
    "print(f\"\\nA:\\n{A}\")\n",
    "print(f\"\\nC: {C}\")\n",
    "print(f\"\\nA + C:\\n{sum_AC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "### Exercise 2.1: Implement Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation: œÉ(z) = 1 / (1 + exp(-z))\"\"\"\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"Tanh activation: tanh(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))\"\"\"\n",
    "    return (torch.exp(z) - torch.exp(-z)) / (torch.exp(z) + torch.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: ReLU(z) = max(0, z)\"\"\"\n",
    "    return torch.maximum(torch.tensor(0.0), z)\n",
    "    # Alternative: torch.clamp(z, min=0)\n",
    "\n",
    "# Test implementations\n",
    "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Sigmoid: {sigmoid(test_input)}\")\n",
    "print(f\"Tanh: {tanh(test_input)}\")\n",
    "print(f\"ReLU: {relu(test_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input range\n",
    "z = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# Apply activation functions\n",
    "sig_output = sigmoid(z)\n",
    "tanh_output = tanh(z)\n",
    "relu_output = relu(z)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z.numpy(), sig_output.numpy(), label='Sigmoid', linewidth=2)\n",
    "plt.plot(z.numpy(), tanh_output.numpy(), label='Tanh', linewidth=2)\n",
    "plt.plot(z.numpy(), relu_output.numpy(), label='ReLU', linewidth=2)\n",
    "plt.xlabel('z', fontsize=12)\n",
    "plt.ylabel('Activation', fontsize=12)\n",
    "plt.title('Activation Functions', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PyTorch built-in functions\n",
    "print(\"Comparing custom implementations with PyTorch built-ins:\")\n",
    "test_z = torch.tensor([0.0, 1.0, -1.0])\n",
    "\n",
    "print(f\"\\nSigmoid - Custom: {sigmoid(test_z)}\")\n",
    "print(f\"Sigmoid - PyTorch: {torch.sigmoid(test_z)}\")\n",
    "print(f\"Match: {torch.allclose(sigmoid(test_z), torch.sigmoid(test_z))}\")\n",
    "\n",
    "print(f\"\\nTanh - Custom: {tanh(test_z)}\")\n",
    "print(f\"Tanh - PyTorch: {torch.tanh(test_z)}\")\n",
    "print(f\"Match: {torch.allclose(tanh(test_z), torch.tanh(test_z))}\")\n",
    "\n",
    "print(f\"\\nReLU - Custom: {relu(test_z)}\")\n",
    "print(f\"ReLU - PyTorch: {torch.relu(test_z)}\")\n",
    "print(f\"Match: {torch.allclose(relu(test_z), torch.relu(test_z))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Derivatives of Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid: œÉ'(z) = œÉ(z) * (1 - œÉ(z))\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"Derivative of tanh: tanh'(z) = 1 - tanh¬≤(z)\"\"\"\n",
    "    t = tanh(z)\n",
    "    return 1 - t**2\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU: ReLU'(z) = 1 if z > 0, else 0\"\"\"\n",
    "    return (z > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation functions and their derivatives\n",
    "z = torch.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(z.numpy(), sigmoid(z).numpy(), label='œÉ(z)', linewidth=2)\n",
    "axes[0].plot(z.numpy(), sigmoid_derivative(z).numpy(), label=\"œÉ'(z)\", linewidth=2, linestyle='--')\n",
    "axes[0].set_title('Sigmoid', fontsize=12)\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Tanh\n",
    "axes[1].plot(z.numpy(), tanh(z).numpy(), label='tanh(z)', linewidth=2)\n",
    "axes[1].plot(z.numpy(), tanh_derivative(z).numpy(), label=\"tanh'(z)\", linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Tanh', fontsize=12)\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# ReLU\n",
    "axes[2].plot(z.numpy(), relu(z).numpy(), label='ReLU(z)', linewidth=2)\n",
    "axes[2].plot(z.numpy(), relu_derivative(z).numpy(), label=\"ReLU'(z)\", linewidth=2, linestyle='--')\n",
    "axes[2].set_title('ReLU', fontsize=12)\n",
    "axes[2].set_xlabel('z')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[2].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigmoid derivative at different points\n",
    "z_values = torch.tensor([-10.0, -5.0, 0.0, 5.0, 10.0])\n",
    "derivatives = sigmoid_derivative(z_values)\n",
    "\n",
    "print(\"Sigmoid derivative at different z values:\")\n",
    "for z, deriv in zip(z_values, derivatives):\n",
    "    print(f\"z = {z:6.1f}, œÉ'(z) = {deriv:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OBSERVATION: Vanishing Gradient Problem\")\n",
    "print(\"=\"*60)\n",
    "print(\"When |z| is large (e.g., z = ¬±10), the gradient is very small (~0.000045).\")\n",
    "print(\"Maximum gradient occurs at z = 0, where œÉ'(0) = 0.25\")\n",
    "print(\"\\nWhy is this a problem?\")\n",
    "print(\"- In deep networks, gradients are multiplied across layers (chain rule)\")\n",
    "print(\"- Small gradients (< 0.25) multiplied many times ‚Üí vanishingly small\")\n",
    "print(\"- Early layers receive almost zero gradient ‚Üí don't learn!\")\n",
    "print(\"\\nReLU helps because:\")\n",
    "print(\"- ReLU'(z) = 1 for z > 0 (no saturation)\")\n",
    "print(\"- Gradients don't vanish for positive activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Propagation by Hand\n",
    "\n",
    "### Exercise 3.1 & 3.2: Manual Computation and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network parameters\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Hidden layer weights and bias\n",
    "W1 = torch.tensor([[0.5, -0.3],\n",
    "                   [0.8, 0.2]])\n",
    "b1 = torch.tensor([0.1, -0.2])\n",
    "\n",
    "# Output layer weights and bias\n",
    "w2 = torch.tensor([1.0, -0.5])\n",
    "b2 = torch.tensor([0.3])\n",
    "\n",
    "print(\"Network parameters:\")\n",
    "print(f\"Input x: {x}\")\n",
    "print(f\"\\nHidden layer:\")\n",
    "print(f\"W1:\\n{W1}\")\n",
    "print(f\"b1: {b1}\")\n",
    "print(f\"\\nOutput layer:\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"b2: {b2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass computation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FORWARD PASS COMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Pre-activation of hidden layer\n",
    "z1 = W1 @ x + b1\n",
    "print(f\"\\nStep 1: z1 = W1 @ x + b1\")\n",
    "print(f\"z1 = {z1}\")\n",
    "print(f\"\\nManual calculation:\")\n",
    "print(f\"z1[0] = 0.5*1 + (-0.3)*2 + 0.1 = {0.5*1 + (-0.3)*2 + 0.1}\")\n",
    "print(f\"z1[1] = 0.8*1 + 0.2*2 + (-0.2) = {0.8*1 + 0.2*2 + (-0.2)}\")\n",
    "\n",
    "# Step 2: Activation of hidden layer (ReLU)\n",
    "a1 = relu(z1)\n",
    "print(f\"\\nStep 2: a1 = ReLU(z1)\")\n",
    "print(f\"a1 = {a1}\")\n",
    "\n",
    "# Step 3: Pre-activation of output layer\n",
    "z2 = w2 @ a1 + b2\n",
    "print(f\"\\nStep 3: z2 = w2 @ a1 + b2\")\n",
    "print(f\"z2 = {z2}\")\n",
    "print(f\"\\nManual calculation:\")\n",
    "print(f\"z2 = 1.0*{a1[0].item()} + (-0.5)*{a1[1].item()} + 0.3 = {1.0*a1[0].item() + (-0.5)*a1[1].item() + 0.3}\")\n",
    "\n",
    "# Step 4: Output (no activation)\n",
    "output = z2\n",
    "print(f\"\\nStep 4: output = z2 (no activation)\")\n",
    "print(f\"Final output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Simple Neural Network\n",
    "\n",
    "### Exercise 4.1: Implement a 2-Layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Simple 2-layer MLP\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input\n",
    "            hidden_dim: Dimension of hidden layer\n",
    "            output_dim: Dimension of output\n",
    "        \"\"\"\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # Layer 1: Linear -> ReLU\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Layer 2: Linear (no activation)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleMLP(input_dim=2, hidden_dim=4, output_dim=1)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Test on XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XOR dataset\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "print(\"Input  | Target\")\n",
    "print(\"-\" * 20)\n",
    "for i in range(len(X)):\n",
    "    print(f\"{X[i].numpy()} | {y[i].item():.0f}\")\n",
    "\n",
    "# Forward pass through untrained network\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "\n",
    "print(\"\\nUntrained Network Predictions:\")\n",
    "print(\"Input  | Prediction | Target | Error\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(X)):\n",
    "    error = abs(predictions[i].item() - y[i].item())\n",
    "    print(f\"{X[i].numpy()} | {predictions[i].item():10.4f} | {y[i].item():6.0f} | {error:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Note: Predictions are random because weights are randomly initialized!\")\n",
    "print(\"Next week, we'll learn how to train the network to solve XOR.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Plot decision boundary of the model\n",
    "    \"\"\"\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    h = 0.01\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Create input tensor from mesh grid\n",
    "    grid_points = torch.tensor(\n",
    "        np.c_[xx.ravel(), yy.ravel()], \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_points)\n",
    "    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    contour = plt.contourf(xx, yy, Z.numpy(), levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "    plt.colorbar(contour, label='Network Output')\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y.squeeze(), cmap='RdYlBu', \n",
    "                         edgecolors='black', s=200, linewidths=2, zorder=10)\n",
    "    \n",
    "    # Add labels for points\n",
    "    for i in range(len(X)):\n",
    "        plt.text(X[i, 0], X[i, 1] + 0.1, f'y={int(y[i].item())}', \n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('x‚ÇÅ', fontsize=12)\n",
    "    plt.ylabel('x‚ÇÇ', fontsize=12)\n",
    "    plt.title('Decision Boundary (Untrained Network)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(model, X, y)\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- The decision boundary is random (untrained weights)\")\n",
    "print(\"- The network cannot solve XOR yet\")\n",
    "print(\"- With training, we can learn the correct non-linear boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4: Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation\n",
    "input_dim = 2\n",
    "hidden_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "# Layer 1: input_dim * hidden_dim + hidden_dim\n",
    "params_layer1 = input_dim * hidden_dim + hidden_dim\n",
    "print(f\"Layer 1 (fc1):\")\n",
    "print(f\"  Weights: {input_dim} √ó {hidden_dim} = {input_dim * hidden_dim}\")\n",
    "print(f\"  Biases: {hidden_dim}\")\n",
    "print(f\"  Total: {params_layer1}\")\n",
    "\n",
    "# Layer 2: hidden_dim * output_dim + output_dim\n",
    "params_layer2 = hidden_dim * output_dim + output_dim\n",
    "print(f\"\\nLayer 2 (fc2):\")\n",
    "print(f\"  Weights: {hidden_dim} √ó {output_dim} = {hidden_dim * output_dim}\")\n",
    "print(f\"  Biases: {output_dim}\")\n",
    "print(f\"  Total: {params_layer2}\")\n",
    "\n",
    "total_params_manual = params_layer1 + params_layer2\n",
    "print(f\"\\nTotal parameters (manual): {total_params_manual}\")\n",
    "\n",
    "# Verify with PyTorch\n",
    "total_params_pytorch = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters (PyTorch): {total_params_pytorch}\")\n",
    "print(f\"\\nMatch: {total_params_manual == total_params_pytorch}\")\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Detailed parameter breakdown:\")\n",
    "print(\"=\"*60)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:15s}: {str(param.shape):15s} -> {param.numel():4d} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrap-up Questions - Solutions\n",
    "\n",
    "### Question 1: Why can't we use a linear activation function in hidden layers?\n",
    "\n",
    "**Answer:** If we use linear activations in hidden layers, the entire network becomes equivalent to a single linear transformation, regardless of depth. This is because:\n",
    "- Composition of linear functions is linear: $f_2(f_1(x)) = W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2)$\n",
    "- We lose the ability to learn non-linear decision boundaries\n",
    "- A deep network with linear activations = shallow linear model\n",
    "- Cannot solve problems like XOR that require non-linear boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Why is ReLU more popular than sigmoid for hidden layers?\n",
    "\n",
    "**Answer:** ReLU has several advantages over sigmoid:\n",
    "1. **No vanishing gradient**: For z > 0, ReLU'(z) = 1 (constant), while sigmoid'(z) ‚â§ 0.25\n",
    "2. **Computational efficiency**: ReLU is just max(0, z) - much faster than exponentials\n",
    "3. **Sparse activation**: About 50% of neurons are zero, leading to sparse representations\n",
    "4. **Empirically better performance**: Trains faster and achieves better results in practice\n",
    "\n",
    "Main disadvantage: \"Dead neurons\" when z ‚â§ 0 always (can be addressed with Leaky ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Parameters for 3-layer MLP with dimensions [10, 50, 50, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [10, 50, 50, 5]\n",
    "\n",
    "total = 0\n",
    "print(\"Layer-wise parameter calculation:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(len(dims) - 1):\n",
    "    weights = dims[i] * dims[i+1]\n",
    "    biases = dims[i+1]\n",
    "    layer_params = weights + biases\n",
    "    total += layer_params\n",
    "    print(f\"Layer {i+1}: {dims[i]:3d} ‚Üí {dims[i+1]:3d}\")\n",
    "    print(f\"  Weights: {dims[i]:3d} √ó {dims[i+1]:3d} = {weights:5d}\")\n",
    "    print(f\"  Biases:  {biases:5d}\")\n",
    "    print(f\"  Total:   {layer_params:5d}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {total}\")\n",
    "\n",
    "# Verify with PyTorch\n",
    "test_model = SimpleMLP(10, 50, 5)\n",
    "# Add another hidden layer to match [10, 50, 50, 5]\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "verify_model = ThreeLayerMLP()\n",
    "pytorch_total = sum(p.numel() for p in verify_model.parameters())\n",
    "print(f\"\\nVerification with PyTorch: {pytorch_total}\")\n",
    "print(f\"Match: {total == pytorch_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Can a single-layer perceptron solve XOR?\n",
    "\n",
    "**Answer:** No, a single-layer perceptron cannot solve XOR. Here's why:\n",
    "\n",
    "1. **Linear decision boundary**: A single-layer perceptron learns: $f(x) = w_1 x_1 + w_2 x_2 + b$\n",
    "   - This defines a line (hyperplane) in the input space\n",
    "   - Decision boundary is where $w_1 x_1 + w_2 x_2 + b = 0$\n",
    "\n",
    "2. **XOR is not linearly separable**: \n",
    "   - Points (0,0) and (1,1) should be on one side (output 0)\n",
    "   - Points (0,1) and (1,0) should be on the other side (output 1)\n",
    "   - No single line can separate these two groups\n",
    "\n",
    "3. **Proof by contradiction**: Try any line - it will always misclassify at least one point\n",
    "\n",
    "4. **Solution**: Need at least one hidden layer with non-linear activation to create non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Single-layer perceptron on XOR\n",
    "class SingleLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "single_layer = SingleLayerPerceptron()\n",
    "\n",
    "print(\"Single-layer perceptron on XOR:\")\n",
    "print(\"\\nNo matter what weights we set, we cannot solve XOR.\")\n",
    "print(\"The decision boundary is always a line, and XOR requires\")\n",
    "print(\"a non-linear boundary to separate the classes.\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.squeeze(), cmap='RdYlBu', \n",
    "           edgecolors='black', s=200, linewidths=2)\n",
    "\n",
    "# Try to draw some lines - none work!\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "plt.plot(x_line, x_line, 'k--', label='Diagonal', linewidth=2)\n",
    "plt.plot(x_line, -x_line + 1, 'r--', label='Anti-diagonal', linewidth=2)\n",
    "plt.plot([0.5, 0.5], [-0.5, 1.5], 'g--', label='Vertical', linewidth=2)\n",
    "plt.plot([-0.5, 1.5], [0.5, 0.5], 'b--', label='Horizontal', linewidth=2)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    plt.text(X[i, 0], X[i, 1] + 0.15, f'y={int(y[i].item())}', \n",
    "            ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlabel('x‚ÇÅ', fontsize=12)\n",
    "plt.ylabel('x‚ÇÇ', fontsize=12)\n",
    "plt.title('No Single Line Can Separate XOR Classes', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways from Week 1:\n",
    "\n",
    "1. **PyTorch Basics**\n",
    "   - Tensors are the fundamental data structure\n",
    "   - Operations like matmul, broadcasting work similarly to NumPy\n",
    "   - Can move tensors to GPU and track gradients\n",
    "\n",
    "2. **Activation Functions**\n",
    "   - Non-linear activations are essential for learning complex patterns\n",
    "   - ReLU is the most popular for hidden layers (no vanishing gradient)\n",
    "   - Sigmoid/tanh have vanishing gradient problems\n",
    "\n",
    "3. **Neural Networks**\n",
    "   - MLPs stack linear layers with non-linear activations\n",
    "   - Forward pass: repeatedly apply Linear ‚Üí Activation\n",
    "   - Depth allows learning hierarchical representations\n",
    "\n",
    "4. **Loss Functions**\n",
    "   - MSE for regression, Cross-entropy for classification\n",
    "   - Have probabilistic interpretations (maximum likelihood)\n",
    "\n",
    "5. **Next Steps**\n",
    "   - Next week: Backpropagation - how to compute gradients efficiently\n",
    "   - Then: Training algorithms to actually optimize the network\n",
    "\n",
    "Great work! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
