{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning - Week 1 Exercises\n",
    "## Neural Network Basics with PyTorch\n",
    "\n",
    "In this notebook, you will:\n",
    "- Get familiar with PyTorch tensors\n",
    "- Implement and visualize activation functions\n",
    "- Understand forward propagation\n",
    "- Build a simple neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Basics (15-20 minutes)\n",
    "\n",
    "### Exercise 1.1: Creating and Manipulating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Create a tensor from a list\n",
    "# TODO: Create a tensor from [1, 2, 3, 4, 5]\n",
    "tensor_a = None  # Replace with your code\n",
    "\n",
    "# Print shape and dtype\n",
    "print(f\"Shape: {tensor_a.shape}\")\n",
    "print(f\"Data type: {tensor_a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Create a 3x4 tensor with random values from standard normal distribution\n",
    "# TODO: Use torch.randn()\n",
    "tensor_b = None  # Replace with your code\n",
    "\n",
    "print(f\"Tensor shape: {tensor_b.shape}\")\n",
    "print(tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Create tensors of ones and zeros\n",
    "# TODO: Create 2x3 tensors\n",
    "ones_tensor = None  # Replace with your code\n",
    "zeros_tensor = None  # Replace with your code\n",
    "\n",
    "print(\"Ones tensor:\")\n",
    "print(ones_tensor)\n",
    "print(\"\\nZeros tensor:\")\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Reshape tensors\n",
    "# TODO: Reshape tensor_b to (2, 6) then to (12, 1)\n",
    "reshaped_1 = None  # tensor_b reshaped to (2, 6)\n",
    "reshaped_2 = None  # tensor_b reshaped to (12, 1)\n",
    "\n",
    "print(f\"Original shape: {tensor_b.shape}\")\n",
    "print(f\"Reshaped to (2, 6): {reshaped_1.shape}\")\n",
    "print(f\"Reshaped to (12, 1): {reshaped_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Dot product\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "w = torch.tensor([0.5, -0.3, 0.8])\n",
    "\n",
    "# TODO: Compute dot product\n",
    "dot_product = None  # Replace with your code\n",
    "\n",
    "print(f\"Dot product: {dot_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Matrix-vector multiplication\n",
    "W = torch.randn(3, 2)\n",
    "x = torch.randn(2)\n",
    "\n",
    "# TODO: Compute W @ x using torch.matmul() or @ operator\n",
    "result = None  # Replace with your code\n",
    "\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Broadcasting\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(2, 3)\n",
    "C = torch.randn(3)\n",
    "\n",
    "# TODO: Element-wise addition\n",
    "sum_AB = None  # A + B\n",
    "sum_AC = None  # A + C (broadcasting)\n",
    "\n",
    "print(f\"A + B shape: {sum_AB.shape}\")\n",
    "print(f\"A + C shape: {sum_AC.shape}\")\n",
    "print(\"\\nBroadcasting example:\")\n",
    "print(f\"A shape: {A.shape}, C shape: {C.shape}\")\n",
    "print(\"C is broadcast to match A's shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions (20 minutes)\n",
    "\n",
    "### Exercise 2.1: Implement Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function: Ïƒ(z) = 1 / (1 + exp(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: Input tensor\n",
    "    Returns:\n",
    "        Output tensor with sigmoid applied element-wise\n",
    "    \"\"\"\n",
    "    # TODO: Implement sigmoid\n",
    "    pass\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Tanh activation function: tanh(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: Input tensor\n",
    "    Returns:\n",
    "        Output tensor with tanh applied element-wise\n",
    "    \"\"\"\n",
    "    # TODO: Implement tanh\n",
    "    pass\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU activation function: ReLU(z) = max(0, z)\n",
    "    \n",
    "    Args:\n",
    "        z: Input tensor\n",
    "    Returns:\n",
    "        Output tensor with ReLU applied element-wise\n",
    "    \"\"\"\n",
    "    # TODO: Implement ReLU\n",
    "    # Hint: Use torch.maximum() or torch.clamp()\n",
    "    pass\n",
    "\n",
    "# Test your implementations\n",
    "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Sigmoid: {sigmoid(test_input)}\")\n",
    "print(f\"Tanh: {tanh(test_input)}\")\n",
    "print(f\"ReLU: {relu(test_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input range\n",
    "z = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# TODO: Apply activation functions\n",
    "sig_output = None  # sigmoid(z)\n",
    "tanh_output = None  # tanh(z)\n",
    "relu_output = None  # relu(z)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z.numpy(), sig_output.numpy(), label='Sigmoid', linewidth=2)\n",
    "plt.plot(z.numpy(), tanh_output.numpy(), label='Tanh', linewidth=2)\n",
    "plt.plot(z.numpy(), relu_output.numpy(), label='ReLU', linewidth=2)\n",
    "plt.xlabel('z', fontsize=12)\n",
    "plt.ylabel('Activation', fontsize=12)\n",
    "plt.title('Activation Functions', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PyTorch built-in functions\n",
    "print(\"Comparing custom implementations with PyTorch built-ins:\")\n",
    "test_z = torch.tensor([0.0, 1.0, -1.0])\n",
    "\n",
    "print(f\"\\nSigmoid - Custom: {sigmoid(test_z)}\")\n",
    "print(f\"Sigmoid - PyTorch: {torch.sigmoid(test_z)}\")\n",
    "\n",
    "print(f\"\\nTanh - Custom: {tanh(test_z)}\")\n",
    "print(f\"Tanh - PyTorch: {torch.tanh(test_z)}\")\n",
    "\n",
    "print(f\"\\nReLU - Custom: {relu(test_z)}\")\n",
    "print(f\"ReLU - PyTorch: {torch.relu(test_z)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Derivatives of Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid: Ïƒ'(z) = Ïƒ(z) * (1 - Ïƒ(z))\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of tanh: tanh'(z) = 1 - tanhÂ²(z)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU: ReLU'(z) = 1 if z > 0, else 0\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # Hint: Use (z > 0).float() to convert boolean to float\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation functions and their derivatives\n",
    "z = torch.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(z.numpy(), sigmoid(z).numpy(), label='Ïƒ(z)', linewidth=2)\n",
    "axes[0].plot(z.numpy(), sigmoid_derivative(z).numpy(), label=\"Ïƒ'(z)\", linewidth=2, linestyle='--')\n",
    "axes[0].set_title('Sigmoid', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[1].plot(z.numpy(), tanh(z).numpy(), label='tanh(z)', linewidth=2)\n",
    "axes[1].plot(z.numpy(), tanh_derivative(z).numpy(), label=\"tanh'(z)\", linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Tanh', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[2].plot(z.numpy(), relu(z).numpy(), label='ReLU(z)', linewidth=2)\n",
    "axes[2].plot(z.numpy(), relu_derivative(z).numpy(), label=\"ReLU'(z)\", linewidth=2, linestyle='--')\n",
    "axes[2].set_title('ReLU', fontsize=12)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigmoid derivative at different points\n",
    "z_values = torch.tensor([-10.0, -5.0, 0.0, 5.0, 10.0])\n",
    "\n",
    "# TODO: Compute sigmoid derivatives\n",
    "derivatives = None  # sigmoid_derivative(z_values)\n",
    "\n",
    "print(\"Sigmoid derivative at different z values:\")\n",
    "for z, deriv in zip(z_values, derivatives):\n",
    "    print(f\"z = {z:6.1f}, Ïƒ'(z) = {deriv:.6f}\")\n",
    "\n",
    "print(\"\\nObservation: When |z| is large, the gradient is very small (close to 0).\")\n",
    "print(\"This is the vanishing gradient problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Questions:**\n",
    "1. What happens to the gradient when |z| is large?\n",
    "2. Why is this a problem for training deep networks?\n",
    "3. How does ReLU help with this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Propagation by Hand (25 minutes)\n",
    "\n",
    "### Exercise 3.1: Manual Computation\n",
    "\n",
    "Network structure:\n",
    "- Input: x = [1, 2]\n",
    "- Hidden layer: 2 neurons, ReLU activation\n",
    "- Output layer: 1 neuron, no activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network parameters\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Hidden layer weights and bias\n",
    "W1 = torch.tensor([[0.5, -0.3],\n",
    "                   [0.8, 0.2]])\n",
    "b1 = torch.tensor([0.1, -0.2])\n",
    "\n",
    "# Output layer weights and bias\n",
    "w2 = torch.tensor([1.0, -0.5])\n",
    "b2 = torch.tensor([0.3])\n",
    "\n",
    "print(\"Network parameters:\")\n",
    "print(f\"Input x: {x}\")\n",
    "print(f\"\\nHidden layer:\")\n",
    "print(f\"W1 shape: {W1.shape}\")\n",
    "print(f\"W1:\\n{W1}\")\n",
    "print(f\"b1: {b1}\")\n",
    "print(f\"\\nOutput layer:\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"b2: {b2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute forward pass step by step\n",
    "\n",
    "# Step 1: Pre-activation of hidden layer\n",
    "z1 = None  # W1 @ x + b1\n",
    "print(f\"z1 (pre-activation hidden): {z1}\")\n",
    "\n",
    "# Step 2: Activation of hidden layer (ReLU)\n",
    "a1 = None  # relu(z1)\n",
    "print(f\"a1 (activation hidden): {a1}\")\n",
    "\n",
    "# Step 3: Pre-activation of output layer\n",
    "z2 = None  # w2 @ a1 + b2\n",
    "print(f\"z2 (pre-activation output): {z2}\")\n",
    "\n",
    "# Step 4: Output (no activation)\n",
    "output = z2\n",
    "print(f\"\\nFinal output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify your calculation by hand:**\n",
    "\n",
    "1. $z_1 = W^{(1)} x + b^{(1)} = \\begin{bmatrix} 0.5 & -0.3 \\\\ 0.8 & 0.2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} = ?$\n",
    "\n",
    "2. $a_1 = \\text{ReLU}(z_1) = ?$\n",
    "\n",
    "3. $z_2 = w^{(2)T} a_1 + b^{(2)} = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Simple Neural Network (30 minutes)\n",
    "\n",
    "### Exercise 4.1: Implement a 2-Layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Simple 2-layer MLP\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input\n",
    "            hidden_dim: Dimension of hidden layer\n",
    "            output_dim: Dimension of output\n",
    "        \"\"\"\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # TODO: Initialize layers\n",
    "        # Hint: Use nn.Linear(in_features, out_features)\n",
    "        self.fc1 = None  # First linear layer\n",
    "        self.fc2 = None  # Second linear layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Layer 1: Linear -> ReLU\n",
    "        # Layer 2: Linear (no activation)\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Create model\n",
    "model = SimpleMLP(input_dim=2, hidden_dim=4, output_dim=1)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Test on XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XOR dataset\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i].numpy()}, Target: {y[i].item()}\")\n",
    "\n",
    "# Forward pass through untrained network\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "\n",
    "print(\"\\nUntrained network predictions:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i].numpy()}, Prediction: {predictions[i].item():.4f}, Target: {y[i].item()}\")\n",
    "\n",
    "print(\"\\nNote: Predictions are random because the network is untrained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Plot decision boundary of the model\n",
    "    \"\"\"\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    h = 0.01\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # TODO: Create input tensor from mesh grid\n",
    "    # Hint: Stack xx.ravel() and yy.ravel(), then convert to torch tensor\n",
    "    grid_points = None  # Shape should be (n_points, 2)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_points)\n",
    "    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z.numpy(), levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "    plt.colorbar(label='Network Output')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.squeeze(), cmap='RdYlBu', \n",
    "                edgecolors='black', s=200, linewidths=2)\n",
    "    \n",
    "    plt.xlabel('x1', fontsize=12)\n",
    "    plt.ylabel('x2', fontsize=12)\n",
    "    plt.title('Decision Boundary (Untrained Network)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4: Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation\n",
    "input_dim = 2\n",
    "hidden_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "# TODO: Calculate number of parameters manually\n",
    "# Layer 1: input_dim * hidden_dim + hidden_dim (weights + biases)\n",
    "# Layer 2: hidden_dim * output_dim + output_dim (weights + biases)\n",
    "\n",
    "params_layer1 = None  # Replace with calculation\n",
    "params_layer2 = None  # Replace with calculation\n",
    "total_params_manual = None  # Replace with calculation\n",
    "\n",
    "print(f\"Layer 1 parameters: {params_layer1}\")\n",
    "print(f\"Layer 2 parameters: {params_layer2}\")\n",
    "print(f\"Total parameters (manual): {total_params_manual}\")\n",
    "\n",
    "# Verify with PyTorch\n",
    "total_params_pytorch = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters (PyTorch): {total_params_pytorch}\")\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"\\nDetailed parameter breakdown:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} -> {param.numel()} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrap-up Questions\n",
    "\n",
    "Answer these questions based on what you've learned:\n",
    "\n",
    "1. **Why can't we use a linear activation function in hidden layers?**\n",
    "   - Your answer: \n",
    "\n",
    "2. **Why is ReLU more popular than sigmoid for hidden layers?**\n",
    "   - Your answer: \n",
    "\n",
    "3. **How many parameters would a 3-layer MLP have with dimensions [10, 50, 50, 5]?**\n",
    "   - Your answer: \n",
    "\n",
    "4. **Can a single-layer perceptron solve XOR? Why or why not?**\n",
    "   - Your answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Calculate parameters for a 3-layer MLP\n",
    "dims = [10, 50, 50, 5]\n",
    "\n",
    "# TODO: Calculate total parameters\n",
    "total = 0\n",
    "for i in range(len(dims) - 1):\n",
    "    layer_params = None  # Calculate params for layer i\n",
    "    total += layer_params\n",
    "    print(f\"Layer {i+1}: {dims[i]} -> {dims[i+1]}, Parameters: {layer_params}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Week Preview\n",
    "\n",
    "Next week we'll learn:\n",
    "- How to compute gradients efficiently using **backpropagation**\n",
    "- How to actually **train** these networks\n",
    "- Different **optimization algorithms** (SGD, Adam, etc.)\n",
    "\n",
    "Make sure you understand:\n",
    "- The chain rule from calculus\n",
    "- Matrix multiplication and dimensions\n",
    "- How to compute gradients of simple functions\n",
    "\n",
    "Great work! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
